"""Fetching IDs for files and datasets."""

from pathlib import Path
from typing import Dict, Union
from uuid import uuid4
from os import environ
from datetime import date
import shortuuid

from .logger import LOG
from ..config import CONFIG_INFO

from httpx import Headers, AsyncClient, Response, DecodingError


def generate_dataset_id(user: str, inbox_path: str, ns: Union[str, None] = None) -> str:
    """Generate accession ID for dataset.

    Generate dataset id based on folder or user.
    We keep the email domain as users might have same name on different domains
    and that could indicate different datasets.
    """
    file_path = Path(inbox_path)
    file_path_parts = file_path.parts
    dataset = ""
    ns = ns if ns else "urn:neic:"

    # add trailing slash if it does not exist
    if ns.startswith(("http://", "https://")):
        ns = ns.rstrip("/") + "/"
    # if a file it is submited in the root directory the dataset
    # is then ns:<username>
    # otherwise we take the root directory and construct the path
    # ns:<username>-<root_dir>
    if len(file_path_parts) <= 2:
        dataset = f"{ns}{user}"
    else:
        # if it is / then we take the next value
        dataset = f"{ns}{user}-{file_path_parts[1]}"

    LOG.debug(f"generated dataset temp id as: {dataset}")

    return dataset


def generate_accession_id() -> str:
    """Generate Stable ID."""
    accessionID = uuid4()

    urn = accessionID.urn
    LOG.debug(f"generated accession id as: {urn}")
    return urn


class DOIHandler:
    """Handler for DOI registration at Datacite.

    The workflow consists of create a short uuid based on user and folder/root directory
    where the file was uploaded. Based on this information we group files into dataset
    It is recommended that first step is to create a draft DOI as it can later be removed easier,
    in case of an error.

    ``create_draft_doi`` generates the identifier using a 10 chars shortuuid from, which guarantee
    uniqueness based on the way we generate the dataset ID.

    The ``set_doi_state`` is dependent on generating a doi_suffix as draft.
    We do this if errors ocurr in registering the resource in REMS
    """

    def __init__(self) -> None:
        """Define DOI credentials and config."""
        self.doi_prefix = environ.get("DOI_PREFIX", "")
        self.doi_api = environ.get("DOI_API", "")
        self.doi_user = environ.get("DOI_USER", "")
        self.doi_key = environ.get("DOI_KEY", "")
        self.ns_url = f"{CONFIG_INFO['datacite']['url'].rstrip('/')}/{self.doi_prefix}"

    async def create_draft_doi(self, user: str, inbox_path: str) -> Union[Dict, None]:
        """Create an auto-generated draft DOI.

        We are using just the prefix for the DOI so that it will be autogenerated.
        """
        dataset = generate_dataset_id(user, inbox_path, self.ns_url)
        suffix = shortuuid.uuid(name=dataset)[:10]
        doi_suffix = f"{suffix[:4]}-{suffix[4:]}"

        headers = Headers({"Content-Type": "application/json"})
        draft_doi_payload = {"data": {"type": "dois", "attributes": {"doi": f"{self.doi_prefix}/{doi_suffix}"}}}
        async with AsyncClient() as client:
            response = await client.post(
                self.doi_api, auth=(self.doi_user, self.doi_key), json=draft_doi_payload, headers=headers
            )
        doi_data = None
        if response.status_code == 201:
            draft_resp = response.json()
            _doi = draft_resp["data"]["attributes"]["doi"]
            _suffix = draft_resp["data"]["attributes"]["suffix"]
            LOG.debug(f"DOI draft created and response was: {draft_resp}")
            LOG.info(f"DOI draft created with doi: {_doi}.")
            doi_data = {
                "suffix": _suffix,
                "fullDOI": _doi,
                "dataset": f"{self.ns_url}/{_suffix.lower()}",
            }
        else:
            LOG.error(f"DOI API create draft request failed with code: {response.status_code}")
            doi_data = self._check_errors(response, doi_suffix)

        return doi_data

    async def set_doi_state(self, state: str, doi_suffix: str) -> Union[Dict, None]:
        """Set DOI and associated metadata.

        :param state: can be publish, register or hide, or even draft if preferred .
        :param doi: DOI to do operations on.
        """
        publish_data_payload = {
            "data": {
                "id": f"{self.doi_prefix}/{doi_suffix}",
                "type": "dois",
                "attributes": {
                    "event": state,
                    "doi": f"{self.doi_prefix}/{doi_suffix}",
                    "titles": [{"title": f"{CONFIG_INFO['datacite']['titlePrefix']} {doi_suffix}", "lang": "en"}],
                    "publisher": CONFIG_INFO["datacite"]["publisher"],
                    "creators": CONFIG_INFO["datacite"]["creators"],
                    # will be current year
                    "publicationYear": date.today().year,
                    # resource type is predefined as dataset
                    "types": {
                        "ris": "DATA",
                        "bibtex": "misc",
                        "citeproc": "dataset",
                        "schemaOrg": "Dataset",
                        "resourceTypeGeneral": "Dataset",
                    },
                    "subjects": CONFIG_INFO["datacite"]["subjects"],
                    "url": CONFIG_INFO["datacite"]["resourceURL"],
                    "schemaVersion": "https://schema.datacite.org/meta/kernel-4.3/",
                },
            }
        }
        headers = Headers({"Content-Type": "application/json"})
        async with AsyncClient() as client:
            response = await client.put(
                f"{self.doi_api}/{self.doi_prefix}/{doi_suffix}",
                auth=(self.doi_user, self.doi_key),
                json=publish_data_payload,
                headers=headers,
            )
        doi_data = None
        if response.status_code == 200:
            publish_resp = response.json()
            _doi = publish_resp["data"]["attributes"]["doi"]
            _suffix = publish_resp["data"]["attributes"]["suffix"]
            LOG.debug(f"DOI created with state: {state} and response was: {publish_resp}")
            LOG.info(f"DOI created: {_doi} with state: {state}.")
            doi_data = {
                "suffix": _suffix,
                "fullDOI": _doi,
                "dataset": f"{self.ns_url}/{_suffix.lower()}",
            }
        else:
            LOG.error(f"DOI API request failed with code: {response.status_code}")
            doi_data = self._check_errors(response, doi_suffix)

        return doi_data

    def _check_errors(self, response: Response, doi_suffix: str) -> Union[Dict, None]:
        try:
            errors_resp = response.json()["errors"]
        except DecodingError:
            LOG.error("Decoding JSON error response was not possible.")
            raise
        except Exception as e:
            LOG.error(f"Unknown exception occured with content: {e}.")
            raise
        else:
            doi_data = None
            if len(errors_resp) == 1:
                error_msg = errors_resp[0]["title"] if "title" in errors_resp[0] else errors_resp[0]["detail"]
                if errors_resp[0]["source"] == "doi" and error_msg == "This DOI has already been taken":
                    LOG.info("DOI already taken, we will associate the submission to this doi dataset.")
                    doi_data = {
                        "suffix": doi_suffix,
                        "fullDOI": f"{self.doi_prefix}/{doi_suffix}",
                        "dataset": f"{self.ns_url}/{doi_suffix.lower()}",
                    }
                else:
                    LOG.error(f"Error occurred: {errors_resp}")
                    raise Exception(f"{error_msg}")
            elif len(errors_resp) > 1:
                LOG.error(f"Multiple errors occurred: {errors_resp}")
                raise Exception(f"Multiple errors occurred: {errors_resp}")
            return doi_data
